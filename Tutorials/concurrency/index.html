
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="Tolga Dimlioglu, Yunfei Teng" name="author"/>
<link href="../materials/" rel="prev"/>
<link href="../../SDTL/codes/" rel="next"/>
<link href="../../assets/images/favicon.png" rel="icon"/>
<meta content="mkdocs-1.4.2, mkdocs-material-9.1.3" name="generator"/>
<title>Concurrency - Direbuted Training Library</title>
<link href="../../assets/stylesheets/main.c4a75a56.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.a0c5b2b5.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            
                .gscrollbar-fixer { padding-right: 15px; }
                .gdesc-inner { font-size: 0.75rem; }
                body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
                body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
                body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
                </style><script src="../../assets/javascripts/glightbox.min.js"></script></head>
<body data-md-color-accent="indigo" data-md-color-primary="indigo" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#concurrency">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<header class="md-header md-header--shadow" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Direbuted Training Library" class="md-header__button md-logo" data-md-component="logo" href="../.." title="Direbuted Training Library">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Direbuted Training Library
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Concurrency
            
          </span>
</div>
</div>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Direbuted Training Library" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="Direbuted Training Library">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg>
</a>
    Direbuted Training Library
  </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../..">
        Introduction
      </a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Tutorials
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_2">
<span class="md-nav__icon md-icon"></span>
          Tutorials
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../materials/">
        Introduction
      </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          Concurrency
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
        Concurrency
      </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#1-concurrency">
    1. Concurrency
  </a>
<nav aria-label="1. Concurrency" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#1a-multithreading">
    (1.a) MultiThreading
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#1b-multiprocessing">
    (1.b) MultiProcessing
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#1c-daemon-thread-reference">
    (1.c) Daemon Thread (Reference)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#what-does-join-function-in-multithreading-do-reference">
    What does join() function in multithreading do? [Reference]
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#2-nvidia-packages">
    2. NVIDIA Packages
  </a>
<nav aria-label="2. NVIDIA Packages" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#2a-core">
    (2.a) CORE
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#2b-cuda-events">
    (2.b) CUDA Events
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#3-discussion">
    3. Discussion
  </a>
<nav aria-label="3. Discussion" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#potential-issues-to-avoid">
    Potential Issues to avoid
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#debugging">
    Debugging
  </a>
<nav aria-label="Debugging" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#distributed-training-operation-tests">
    Distributed Training Operation Tests
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          SDTL
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
          SDTL
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../SDTL/codes/">
        Codes
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../SDTL/readme/">
        ReadMe
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          ASDTL
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_4_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
          ASDTL
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../ASDTL/architecture/">
        Architecture
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../ASDTL/comm/">
        Commmunication
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../machine/">
        Environment
      </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#1-concurrency">
    1. Concurrency
  </a>
<nav aria-label="1. Concurrency" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#1a-multithreading">
    (1.a) MultiThreading
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#1b-multiprocessing">
    (1.b) MultiProcessing
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#1c-daemon-thread-reference">
    (1.c) Daemon Thread (Reference)
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#what-does-join-function-in-multithreading-do-reference">
    What does join() function in multithreading do? [Reference]
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#2-nvidia-packages">
    2. NVIDIA Packages
  </a>
<nav aria-label="2. NVIDIA Packages" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#2a-core">
    (2.a) CORE
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#2b-cuda-events">
    (2.b) CUDA Events
  </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#3-discussion">
    3. Discussion
  </a>
<nav aria-label="3. Discussion" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#potential-issues-to-avoid">
    Potential Issues to avoid
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#debugging">
    Debugging
  </a>
<nav aria-label="Debugging" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#distributed-training-operation-tests">
    Distributed Training Operation Tests
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1 id="concurrency">Concurrency<a class="headerlink" href="#concurrency" title="Permanent link">¶</a></h1>
<h2 id="1-concurrency"><strong>1. Concurrency</strong><a class="headerlink" href="#1-concurrency" title="Permanent link">¶</a></h2>
<h3 id="1a-multithreading">(1.a) MultiThreading<a class="headerlink" href="#1a-multithreading" title="Permanent link">¶</a></h3>
<ul>
<li>CPUs are added for increasing computing power.</li>
<li>Many processes are executed <strong>simultaneously</strong>.</li>
<li>Every process owned a separate address space.</li>
</ul>
<h3 id="1b-multiprocessing">(1.b) MultiProcessing<a class="headerlink" href="#1b-multiprocessing" title="Permanent link">¶</a></h3>
<ul>
<li>Many threads are created of a single process for increasing computing power.</li>
<li>Many threads of a process are executed <strong>sequentially</strong> due to <em>Global Interpretation Lock (GIL)</em>.</li>
<li>All threads shared a common address space.</li>
</ul>
<h3 id="1c-daemon-thread-reference">(1.c) Daemon Thread <a href="https://docs.python.org/2/library/threading.html">(Reference)</a><a class="headerlink" href="#1c-daemon-thread-reference" title="Permanent link">¶</a></h3>
<p>A thread can be flagged as a “daemon thread”. The significance of this flag is that the entire Python program exits when only daemon threads are left. The initial value is inherited from the creating thread. The flag can be set through the daemon property.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Daemon threads are abruptly stopped at shutdown. Their resources (such as open files, database transactions, etc.) may not be released properly. If you want your threads to stop gracefully, make them non-daemonic and use a suitable signalling mechanism such as an <code>Event</code>.</p>
</div>
<p>There is a <strong>“main thread”</strong> object; this corresponds to the <strong>initial thread</strong> of control in the Python program. It is not a daemon thread. Its initial value is inherited from the creating thread; the main thread is not a daemon thread and therefore all threads created in the main thread default to daemon = False.</p>
<h3 id="what-does-join-function-in-multithreading-do-reference">What does <strong>join()</strong> function in multithreading do? <a href="https://stackoverflow.com/questions/15085348/what-is-the-use-of-join-in-python-threading">[Reference]</a><a class="headerlink" href="#what-does-join-function-in-multithreading-do-reference" title="Permanent link">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The following two methods have different behaviors:</p>
<p><strong>Parallel one:</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a href="#__codelineno-0-1" id="__codelineno-0-1" name="__codelineno-0-1"></a><span class="n">task1</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span id="__span-0-2"><a href="#__codelineno-0-2" id="__codelineno-0-2" name="__codelineno-0-2"></a><span class="n">task2</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span id="__span-0-3"><a href="#__codelineno-0-3" id="__codelineno-0-3" name="__codelineno-0-3"></a><span class="n">task1</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span><span id="__span-0-4"><a href="#__codelineno-0-4" id="__codelineno-0-4" name="__codelineno-0-4"></a><span class="n">taks2</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></code></pre></div>
<strong>Sequetial one</strong>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a href="#__codelineno-1-1" id="__codelineno-1-1" name="__codelineno-1-1"></a><span class="n">task1</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span id="__span-1-2"><a href="#__codelineno-1-2" id="__codelineno-1-2" name="__codelineno-1-2"></a><span class="n">task1</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span><span id="__span-1-3"><a href="#__codelineno-1-3" id="__codelineno-1-3" name="__codelineno-1-3"></a><span class="n">task2</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span id="__span-1-4"><a href="#__codelineno-1-4" id="__codelineno-1-4" name="__codelineno-1-4"></a><span class="n">taks2</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</span></code></pre></div>
<code>join()</code> has to be done after every thread has been started.</p>
</div>
<h2 id="2-nvidia-packages"><strong>2. NVIDIA Packages</strong><a class="headerlink" href="#2-nvidia-packages" title="Permanent link">¶</a></h2>
<h3 id="2a-core">(2.a) CORE<a class="headerlink" href="#2a-core" title="Permanent link">¶</a></h3>
<ul>
<li>PyTorch Distributed Training: <a href="https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py">[Data Parallel Example]</a></li>
<li>PyTorch Distributed Training Operations: <a href="https://github.com/pytorch/pytorch/blob/master/torch/distributed/distributed_c10d.py">[distributed_c10]</a></li>
<li>PyTorch NCCL: <a href="https://github.com/pytorch/pytorch/blob/792cb774f152bab5b968f4ec51da0fc21ff9e895/torch/lib/c10d/ProcessGroupNCCL.cpp">[NCCL Library]</a></li>
<li>CUDA Events: <a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html">[CUDA Event]</a></li>
<li>CUDA Semantics: <a href="https://pytorch.org/docs/stable/notes/cuda.html">[CUDA Semantics]</a></li>
</ul>
<h3 id="2b-cuda-events">(2.b) CUDA Events<a class="headerlink" href="#2b-cuda-events" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>SynChronous Case
    <div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a href="#__codelineno-2-1" id="__codelineno-2-1" name="__codelineno-2-1"></a><span class="sd">'''Waiting on the work's corresponding CUDA events'''</span>
</span><span id="__span-2-2"><a href="#__codelineno-2-2" id="__codelineno-2-2" name="__codelineno-2-2"></a><span class="n">void</span> <span class="n">ProcessGroupNCCL</span><span class="p">::</span><span class="n">WorkNCCL</span><span class="p">::</span><span class="n">synchronize</span><span class="p">()</span> <span class="p">{</span>
</span><span id="__span-2-3"><a href="#__codelineno-2-3" id="__codelineno-2-3" name="__codelineno-2-3"></a>  <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">devices_</span><span class="o">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span><span id="__span-2-4"><a href="#__codelineno-2-4" id="__codelineno-2-4" name="__codelineno-2-4"></a>    <span class="n">auto</span> <span class="n">currentStream</span> <span class="o">=</span> <span class="n">at</span><span class="p">::</span><span class="n">cuda</span><span class="p">::</span><span class="n">getCurrentCUDAStream</span><span class="p">(</span><span class="n">devices_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">());</span>
</span><span id="__span-2-5"><a href="#__codelineno-2-5" id="__codelineno-2-5" name="__codelineno-2-5"></a><span class="w">    </span><span class="sd">'''Block the current stream on the NCCL stream'''</span>
</span><span id="__span-2-6"><a href="#__codelineno-2-6" id="__codelineno-2-6" name="__codelineno-2-6"></a>    <span class="n">cudaEvents_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">block</span><span class="p">(</span><span class="n">currentStream</span><span class="p">);</span>
</span><span id="__span-2-7"><a href="#__codelineno-2-7" id="__codelineno-2-7" name="__codelineno-2-7"></a><span class="w">    </span><span class="sd">'''If we use the work to do barrier, we should block here'''</span>
</span><span id="__span-2-8"><a href="#__codelineno-2-8" id="__codelineno-2-8" name="__codelineno-2-8"></a>    <span class="k">if</span> <span class="p">(</span><span class="err">!</span><span class="n">barrierTensors_</span><span class="o">.</span><span class="n">empty</span><span class="p">())</span> <span class="p">{</span>
</span><span id="__span-2-9"><a href="#__codelineno-2-9" id="__codelineno-2-9" name="__codelineno-2-9"></a>      <span class="n">at</span><span class="p">::</span><span class="n">cuda</span><span class="p">::</span><span class="n">CUDAGuard</span> <span class="n">gpuGuard</span><span class="p">(</span><span class="n">devices_</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span><span id="__span-2-10"><a href="#__codelineno-2-10" id="__codelineno-2-10" name="__codelineno-2-10"></a>      <span class="n">AT_CUDA_CHECK</span><span class="p">(</span><span class="n">cudaDeviceSynchronize</span><span class="p">());</span>
</span><span id="__span-2-11"><a href="#__codelineno-2-11" id="__codelineno-2-11" name="__codelineno-2-11"></a>    <span class="p">}</span>
</span><span id="__span-2-12"><a href="#__codelineno-2-12" id="__codelineno-2-12" name="__codelineno-2-12"></a>  <span class="p">}</span>
</span><span id="__span-2-13"><a href="#__codelineno-2-13" id="__codelineno-2-13" name="__codelineno-2-13"></a><span class="p">}</span>
</span></code></pre></div></p>
</li>
<li>
<p>Asynchronous Case
    <div class="language-python3 highlight"><pre><span></span><code><span id="__span-3-1"><a href="#__codelineno-3-1" id="__codelineno-3-1" name="__codelineno-3-1"></a><span class="nb">bool</span> <span class="n">ProcessGroupNCCL</span><span class="p">::</span><span class="n">WorkNCCL</span><span class="p">::</span><span class="n">finishedGPUExecution</span><span class="p">()</span> <span class="p">{</span>
</span><span id="__span-3-2"><a href="#__codelineno-3-2" id="__codelineno-3-2" name="__codelineno-3-2"></a>  <span class="k">for</span> <span class="p">(</span><span class="n">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">devices_</span><span class="o">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
</span><span id="__span-3-3"><a href="#__codelineno-3-3" id="__codelineno-3-3" name="__codelineno-3-3"></a><span class="w">    </span><span class="sd">'''Checking the work's corresponding CUDA events' status'''</span>
</span><span id="__span-3-4"><a href="#__codelineno-3-4" id="__codelineno-3-4" name="__codelineno-3-4"></a>    <span class="n">auto</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">cudaEventQuery</span><span class="p">(</span><span class="n">cudaEvents_</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
</span><span id="__span-3-5"><a href="#__codelineno-3-5" id="__codelineno-3-5" name="__codelineno-3-5"></a>    <span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">cudaSuccess</span> <span class="o">&amp;&amp;</span> <span class="n">ret</span> <span class="o">!=</span> <span class="n">cudaErrorNotReady</span><span class="p">)</span> <span class="p">{</span>
</span><span id="__span-3-6"><a href="#__codelineno-3-6" id="__codelineno-3-6" name="__codelineno-3-6"></a>      <span class="n">AT_CUDA_CHECK</span><span class="p">(</span><span class="n">ret</span><span class="p">);</span>
</span><span id="__span-3-7"><a href="#__codelineno-3-7" id="__codelineno-3-7" name="__codelineno-3-7"></a>    <span class="p">}</span>
</span><span id="__span-3-8"><a href="#__codelineno-3-8" id="__codelineno-3-8" name="__codelineno-3-8"></a>    <span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">==</span> <span class="n">cudaErrorNotReady</span><span class="p">)</span> <span class="p">{</span>
</span><span id="__span-3-9"><a href="#__codelineno-3-9" id="__codelineno-3-9" name="__codelineno-3-9"></a>      <span class="k">return</span> <span class="n">false</span><span class="p">;</span>
</span><span id="__span-3-10"><a href="#__codelineno-3-10" id="__codelineno-3-10" name="__codelineno-3-10"></a>    <span class="p">}</span>
</span><span id="__span-3-11"><a href="#__codelineno-3-11" id="__codelineno-3-11" name="__codelineno-3-11"></a>  <span class="p">}</span>
</span><span id="__span-3-12"><a href="#__codelineno-3-12" id="__codelineno-3-12" name="__codelineno-3-12"></a>  <span class="k">return</span> <span class="n">true</span><span class="p">;</span>
</span><span id="__span-3-13"><a href="#__codelineno-3-13" id="__codelineno-3-13" name="__codelineno-3-13"></a><span class="p">}</span>
</span></code></pre></div></p>
</li>
</ul>
<div class="admonition help">
<p class="admonition-title">Help</p>
<ul>
<li>If NCCL doesnot work, try to use MPI and disable NCCL <a href="https://devblogs.nvidia.com/introduction-cuda-aware-mpi/">[cuda aware mpi]</a>.</li>
<li><code>nvidia-smi</code> and <code>cuda device</code> inconsistency. (<a href="https://discuss.pytorch.org/t/gpu-devices-nvidia-smi-and-cuda-get-device-name-output-appear-inconsistent/13150">https://discuss.pytorch.org/t/gpu-devices-nvidia-smi-and-cuda-get-device-name-output-appear-inconsistent/13150</a>)</li>
</ul>
</div>
<h2 id="3-discussion"><strong>3. Discussion</strong><a class="headerlink" href="#3-discussion" title="Permanent link">¶</a></h2>
<h3 id="potential-issues-to-avoid">Potential Issues to avoid<a class="headerlink" href="#potential-issues-to-avoid" title="Permanent link">¶</a></h3>
<ul>
<li>
<p>Using <em>multithreading</em> for synchronous experiments as all the processes can share the memory with each other. However, for asynchronous experiments this will surely not be appreciated due the the low efficiency.</p>
</li>
<li>
<p>Be careful about concurrency of Python as this script language is tricky. When there is some serious bug it may not report anything but pretend everything is OK.</p>
</li>
<li>
<p>Should use <code>watch -n 0.1 nvidia-smi</code> to monitor the performance of GPUs. <em>Loading data</em> usually takes much more time than expected.</p>
</li>
<li>
<p>Should not call <code>torch.cuda.*</code> before spwaning subprocesses. e.g. <code>torch.cuda.device_count()</code>. The explantion is right here <a href="https://github.com/pytorch/pytorch/issues/15734">(report useful error)</a>.</p>
</li>
<li>
<p>"Each subprocess should be assingned an individual GPU to make use of NCCL." This is the suggestion proporsed in this issue (<a href="https://github.com/pytorch/pytorch/issues/15051">https://github.com/pytorch/pytorch/issues/15051</a>). However, based on my test even if spawning 4 processes on the same GPU NCCL still works fine.</p>
</li>
<li>
<p>TCP/IP port racing problem was fixed in PyTorch 1.0.1 version. Before that version, running distribution initialization on <strong>master</strong> machine is mandatory.</p>
</li>
</ul>
<h3 id="debugging">Debugging<a class="headerlink" href="#debugging" title="Permanent link">¶</a></h3>
<h4 id="distributed-training-operation-tests">Distributed Training Operation Tests<a class="headerlink" href="#distributed-training-operation-tests" title="Permanent link">¶</a></h4>
<p>Run <code>distributed_training_test()</code> and check the results of all tests.
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a href="#__codelineno-4-1" id="__codelineno-4-1" name="__codelineno-4-1"></a><span class="c1"># define waiting function </span>
</span><span id="__span-4-2"><a href="#__codelineno-4-2" id="__codelineno-4-2" name="__codelineno-4-2"></a><span class="k">def</span> <span class="nf">dumb_wait</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">"asynchronization"</span><span class="p">):</span>
</span><span id="__span-4-3"><a href="#__codelineno-4-3" id="__codelineno-4-3" name="__codelineno-4-3"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">"Entering "</span> <span class="o">+</span> <span class="n">name</span><span class="p">)</span>
</span><span id="__span-4-4"><a href="#__codelineno-4-4" id="__codelineno-4-4" name="__codelineno-4-4"></a>    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
</span><span id="__span-4-5"><a href="#__codelineno-4-5" id="__codelineno-4-5" name="__codelineno-4-5"></a>    <span class="k">while</span><span class="p">(</span><span class="ow">not</span> <span class="n">req</span><span class="o">.</span><span class="n">is_completed</span><span class="p">()):</span>
</span><span id="__span-4-6"><a href="#__codelineno-4-6" id="__codelineno-4-6" name="__codelineno-4-6"></a>        <span class="k">pass</span>
</span><span id="__span-4-7"><a href="#__codelineno-4-7" id="__codelineno-4-7" name="__codelineno-4-7"></a>    <span class="n">async_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
</span><span id="__span-4-8"><a href="#__codelineno-4-8" id="__codelineno-4-8" name="__codelineno-4-8"></a>    <span class="nb">print</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s2">" spent </span><span class="si">{async_time:.5f}</span><span class="s2"> seconds"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">async_time</span><span class="o">=</span><span class="n">async_time</span><span class="p">))</span>
</span><span id="__span-4-9"><a href="#__codelineno-4-9" id="__codelineno-4-9" name="__codelineno-4-9"></a>
</span><span id="__span-4-10"><a href="#__codelineno-4-10" id="__codelineno-4-10" name="__codelineno-4-10"></a><span class="c1"># define necessary things for testing </span>
</span><span id="__span-4-11"><a href="#__codelineno-4-11" id="__codelineno-4-11" name="__codelineno-4-11"></a><span class="k">def</span> <span class="nf">distributed_training_test</span><span class="p">():</span>
</span><span id="__span-4-12"><a href="#__codelineno-4-12" id="__codelineno-4-12" name="__codelineno-4-12"></a>    <span class="c1"># Synchronous distribution training test</span>
</span><span id="__span-4-13"><a href="#__codelineno-4-13" id="__codelineno-4-13" name="__codelineno-4-13"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">[Synchronous distribution testing...]"</span><span class="p">)</span>
</span><span id="__span-4-14"><a href="#__codelineno-4-14" id="__codelineno-4-14" name="__codelineno-4-14"></a>
</span><span id="__span-4-15"><a href="#__codelineno-4-15" id="__codelineno-4-15" name="__codelineno-4-15"></a>    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-4-16"><a href="#__codelineno-4-16" id="__codelineno-4-16" name="__codelineno-4-16"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed broadcast test"</span><span class="p">)</span>
</span><span id="__span-4-17"><a href="#__codelineno-4-17" id="__codelineno-4-17" name="__codelineno-4-17"></a>
</span><span id="__span-4-18"><a href="#__codelineno-4-18" id="__codelineno-4-18" name="__codelineno-4-18"></a>    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</span><span id="__span-4-19"><a href="#__codelineno-4-19" id="__codelineno-4-19" name="__codelineno-4-19"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed all_reduce test"</span><span class="p">)</span>
</span><span id="__span-4-20"><a href="#__codelineno-4-20" id="__codelineno-4-20" name="__codelineno-4-20"></a>
</span><span id="__span-4-21"><a href="#__codelineno-4-21" id="__codelineno-4-21" name="__codelineno-4-21"></a>    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
</span><span id="__span-4-22"><a href="#__codelineno-4-22" id="__codelineno-4-22" name="__codelineno-4-22"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed all_gather test"</span><span class="p">)</span>
</span><span id="__span-4-23"><a href="#__codelineno-4-23" id="__codelineno-4-23" name="__codelineno-4-23"></a>
</span><span id="__span-4-24"><a href="#__codelineno-4-24" id="__codelineno-4-24" name="__codelineno-4-24"></a>    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast_multigpu</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-4-25"><a href="#__codelineno-4-25" id="__codelineno-4-25" name="__codelineno-4-25"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed broadcast_multigpu test"</span><span class="p">)</span>
</span><span id="__span-4-26"><a href="#__codelineno-4-26" id="__codelineno-4-26" name="__codelineno-4-26"></a>
</span><span id="__span-4-27"><a href="#__codelineno-4-27" id="__codelineno-4-27" name="__codelineno-4-27"></a>    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">([</span><span class="n">tensor</span><span class="p">])</span>
</span><span id="__span-4-28"><a href="#__codelineno-4-28" id="__codelineno-4-28" name="__codelineno-4-28"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed all_reduce_multigpu test"</span><span class="p">)</span>
</span><span id="__span-4-29"><a href="#__codelineno-4-29" id="__codelineno-4-29" name="__codelineno-4-29"></a>
</span><span id="__span-4-30"><a href="#__codelineno-4-30" id="__codelineno-4-30" name="__codelineno-4-30"></a>    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather_multigpu</span><span class="p">([</span><span class="n">tensor_list</span><span class="p">],</span> <span class="p">[</span><span class="n">tensor</span><span class="p">])</span>
</span><span id="__span-4-31"><a href="#__codelineno-4-31" id="__codelineno-4-31" name="__codelineno-4-31"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed all_gather_multigpu test"</span><span class="p">)</span>
</span><span id="__span-4-32"><a href="#__codelineno-4-32" id="__codelineno-4-32" name="__codelineno-4-32"></a>
</span><span id="__span-4-33"><a href="#__codelineno-4-33" id="__codelineno-4-33" name="__codelineno-4-33"></a>    <span class="c1"># Asynchronous distribution training test</span>
</span><span id="__span-4-34"><a href="#__codelineno-4-34" id="__codelineno-4-34" name="__codelineno-4-34"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">[ASynchronous distribution testing...]"</span><span class="p">)</span>
</span><span id="__span-4-35"><a href="#__codelineno-4-35" id="__codelineno-4-35" name="__codelineno-4-35"></a>    <span class="n">req</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-4-36"><a href="#__codelineno-4-36" id="__codelineno-4-36" name="__codelineno-4-36"></a>    <span class="n">dumb_wait</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
</span><span id="__span-4-37"><a href="#__codelineno-4-37" id="__codelineno-4-37" name="__codelineno-4-37"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed broadcast test"</span><span class="p">)</span>
</span><span id="__span-4-38"><a href="#__codelineno-4-38" id="__codelineno-4-38" name="__codelineno-4-38"></a>
</span><span id="__span-4-39"><a href="#__codelineno-4-39" id="__codelineno-4-39" name="__codelineno-4-39"></a>    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-4-40"><a href="#__codelineno-4-40" id="__codelineno-4-40" name="__codelineno-4-40"></a>    <span class="n">dumb_wait</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
</span><span id="__span-4-41"><a href="#__codelineno-4-41" id="__codelineno-4-41" name="__codelineno-4-41"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed all_reduce test"</span><span class="p">)</span>
</span><span id="__span-4-42"><a href="#__codelineno-4-42" id="__codelineno-4-42" name="__codelineno-4-42"></a>
</span><span id="__span-4-43"><a href="#__codelineno-4-43" id="__codelineno-4-43" name="__codelineno-4-43"></a>    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-4-44"><a href="#__codelineno-4-44" id="__codelineno-4-44" name="__codelineno-4-44"></a>    <span class="n">dumb_wait</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
</span><span id="__span-4-45"><a href="#__codelineno-4-45" id="__codelineno-4-45" name="__codelineno-4-45"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed all_gather test"</span><span class="p">)</span>
</span><span id="__span-4-46"><a href="#__codelineno-4-46" id="__codelineno-4-46" name="__codelineno-4-46"></a>
</span><span id="__span-4-47"><a href="#__codelineno-4-47" id="__codelineno-4-47" name="__codelineno-4-47"></a>    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">broadcast_multigpu</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-4-48"><a href="#__codelineno-4-48" id="__codelineno-4-48" name="__codelineno-4-48"></a>    <span class="n">dumb_wait</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
</span><span id="__span-4-49"><a href="#__codelineno-4-49" id="__codelineno-4-49" name="__codelineno-4-49"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed broadcast_multigpu test"</span><span class="p">)</span>
</span><span id="__span-4-50"><a href="#__codelineno-4-50" id="__codelineno-4-50" name="__codelineno-4-50"></a>
</span><span id="__span-4-51"><a href="#__codelineno-4-51" id="__codelineno-4-51" name="__codelineno-4-51"></a>    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce_multigpu</span><span class="p">([</span><span class="n">tensor</span><span class="p">],</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-4-52"><a href="#__codelineno-4-52" id="__codelineno-4-52" name="__codelineno-4-52"></a>    <span class="n">dumb_wait</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
</span><span id="__span-4-53"><a href="#__codelineno-4-53" id="__codelineno-4-53" name="__codelineno-4-53"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed all_reduce_multigpu test"</span><span class="p">)</span>
</span><span id="__span-4-54"><a href="#__codelineno-4-54" id="__codelineno-4-54" name="__codelineno-4-54"></a>
</span><span id="__span-4-55"><a href="#__codelineno-4-55" id="__codelineno-4-55" name="__codelineno-4-55"></a>    <span class="n">dist</span><span class="o">.</span><span class="n">all_gather_multigpu</span><span class="p">([</span><span class="n">tensor_list</span><span class="p">],</span> <span class="p">[</span><span class="n">tensor</span><span class="p">],</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-4-56"><a href="#__codelineno-4-56" id="__codelineno-4-56" name="__codelineno-4-56"></a>    <span class="n">dumb_wait</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>
</span><span id="__span-4-57"><a href="#__codelineno-4-57" id="__codelineno-4-57" name="__codelineno-4-57"></a>    <span class="nb">print</span><span class="p">(</span><span class="s2">" -Passed all_gather_multigpu test"</span><span class="p">)</span>
</span></code></pre></div></p>
<div class="admonition bug">
<p class="admonition-title">GPU parallelism bug</p>
<p>For some specific GPUs, there might be some problem of parallelism and see this <a href="https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158">discussion</a> for solution.</p>
</div>
</article>
</div>
<script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "content.code.select", "content.code.annotate", "content.tabs.link"], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
<script src="../../assets/javascripts/bundle.efa0ade1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body>
</html>